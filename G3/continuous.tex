\chapter{Distributions continues}

\section{Créer des nouvelles distributions}

\subsection{Multiplication par une constante}

\begin{theoreme}{}{}
	Soit $X$, une variable aléatoire continue. Soit $Y = \theta X$ avec $\theta > 0$. Alors, 
	$$F_Y(y) = F_X\left(\frac{y}{\theta}\right) \quad \text{et} \quad f_Y(y) = \frac{1}{\theta} f_X\left(\frac{y}{\theta}\right)$$
\end{theoreme}

Le paramètre $\theta$ est un paramètre d'échelle pour la variable aléatoire $Y.$

\subsection{Création de distributions en élevant à une puissance}

\begin{theoreme}{}{}
	Soit $X$, une variable aléatoire continue et $F_X(0) = 0$. Soit $Y = X^{1/\tau}$. Alors, 
	\begin{itemize}
		\item si $\tau > 0$, on a 
		$$F_Y(y) = F_X\left(y^\tau\right) \quad \text{et} \quad f_Y(y) = \tau y^{\tau - 1} f_{X}\left(y^\tau\right), \quad y>0;$$
		\item si $\tau < 0 $, on a 
		$$F_Y(y) = 1-F_X\left(y^\tau\right) \quad \text{et} \quad f_Y(y) = -\tau y^{\tau - 1} f_{X}\left(y^\tau\right), \quad y>0.$$
	\end{itemize}
\end{theoreme}

\begin{itemize}
	\item Lorsqu'on prend une distribution avec puissance $ \tau > 0$, elle est appelée transformée.
	\item Lorsqu'on prend une distribution avec puissance $ \tau = -1$, elle est appelée inverse.
	\item Lorsqu'on prend une distribution avec puissance $ \tau < 0, \tau \neq -1$, elle est appelée inverse-transformée.
\end{itemize}

\subsection{Création de distributions avec exponentiation}

\begin{theoreme}{}{}
	Soit $X$, une variable aléatoire continue et $f_{X}(x) > 0$ sur le domaine de $x$. Soit $Y = e^X$. Alors, pour $y>0$, on a 
	$$F_{Y}(y) = F_{X}(\ln y) \quad \text{et} \quad f_{Y}(y) = \frac{1}{y}f_{X}(\ln y).$$
\end{theoreme}

\subsection{Mélange}

\begin{theoreme}{}{}
	Soit $X$, une variable aléatoire avec fonction de densité $f_{X\vert \Lambda}(x\vert \lambda)$ et fonction de répartition $F_{X\vert \Lambda}(x\vert \lambda)$, où $\lambda$ est un paramètre de $X$. La fonction de densité inconditionnelle de $X$ est 
	$$f_{X}(x) = \int f_{X\vert\Lambda}(x\vert \lambda) f_{\Lambda}(\lambda) d\lambda$$
	et la fonction de répartition est 
	$$F_{X}(x) = \int F_{X\vert \Lambda}(x\vert\lambda) f_{\Lambda}(\lambda) d\lambda.$$
\end{theoreme}

Autres résultats :
\begin{itemize}
	\item $\displaystyle E\left[X^k\right] = E_{\Lambda}\left[E\left(X^k \vert \Lambda\right)\right]$
	\item $\displaystyle Var(X) = E\left[Var(X \vert \Lambda)\right] + Var\left(E\left[X \vert \Lambda\right]\right)$
\end{itemize}

Les modèles de mélange tendent à créer des distributions à queue lourde. En particulier, si la fonction de hasard de $f_{X\vert\Lambda}$ est décroissante pour tout $\lambda$, la fonction de hasard sera aussi décroissante. 

\subsection{Modèles à fragilité}

Mise en place : 

\begin{itemize}
	\item Soit une variable aléatoire de fragilité $\Lambda>0$
	\item Soit une fonction de hasard conditionnelle $h_{X\vert \Lambda}(x\vert\lambda) = \lambda a(x)$, où $a(x)$ est une fonction connue. 
	\item La fragilité quantifie l'incertitude de la fonction de hasard.  
\end{itemize}

La fonction de survie conditionnelle de $X\vert \Lambda$ est 
$$S_{X\vert\Lambda}(x\vert\lambda) = \exp\left\{-\int_{0}^{x}h_{X\vert\Lambda}(t\vert\lambda)dt\right\} = e^{-\lambda A(x)},$$
où $A(x) = \int_{0}^{x}a(t) dt$. Alors, la fonction de survie inconditionnelle est donnée par 
$$S_{X}(x) = E\left[e^{-\Lambda A(x)}\right] = M_{\Lambda}(-A(x))$$

\subsection{Raccordement de distributions connues}

Si plusieurs processus séparés sont responsables pour générer les pertes

\begin{definition}{}{}
	Une distribution de raccordement à $k$ composantes a une fonction de densité qui peut être exprimé sous la forme
	$$f_{X}(x) = \begin{cases}
	a_1 f_{1}(x), & c_0 \leq x \leq c_1\\
	a_2 f_{2}(x), & c_1 \leq x \leq c_2\\
	\vdots & \vdots\\
	a_k f_{k}(x), & c_{k-1} \leq x \leq c_k
	\end{cases}$$
	Pour $j = 1, 2, \dots, k$, chaque $a_j$ soit être positif, $f_{j}$ doit être une fonction de densité avec toute sa masse sur $(c_{j-1}, c_j)$ et $a_1 + a_2 + \dots + a_k = 1$.
\end{definition}

\section{Familles de distributions et leurs liens}

Connaître les familles de distribution beta transformée et gamma inverse/transformée.

\subsection{Distributions limites}

On peut parfois comparer les familles des distributions basé sur des cas particuliers des familles. Dans d'autres situations, on doit étudier les distributions quand des paramètres tendent vers 0 ou l'infini.

Exemples : 

\begin{itemize}
	\item La distribution gamma est un cas limite de la distribution beta transformée avec $\theta \to \infty, \alpha \to \infty$ et $\frac{\theta}{\alpha^{1/\gamma}} \to \xi$, une constante. 
\end{itemize}

\section{Théorie des valeurs extrêmes}

\subsection{Distributions à valeur extrêmes (DVE)}

\begin{definition}{La distribution Gumbel}{}
	La distribution Gumbel standard a la fonction de répartition
	$$F(x) = G_{0}(x) = \exp \left[-\exp(-x)\right], \quad -\infty <x<\infty.$$
	\tcblower
	Avec les paramètres de location et d'échelle, on a 
	$$F(x) = G_{0, \mu, \theta}(x) = \exp \left[-\exp\left(-\frac{x-\mu}{\theta}\right)\right], \quad -\infty <x<\infty, \theta>0.$$
\end{definition}

\begin{definition}{La distribution de Fréchet}{}
	La distribution de Fréchet standard a la fonction de répartition
	$$F(x) = G_{1, \alpha}(x) = \exp\left(-x^{-\alpha}\right), \quad x\geq 0, \alpha > 0,$$
	où $\alpha$ est un paramètre de forme. 
	\tcblower
	Avec les paramètres de location et d'échelle, on a
	$$F(x) = G_{1, \alpha, \mu, \theta}(x) = \exp\left[-\left(\frac{x-\mu}{\theta}\right)^{-\alpha}\right], x\geq \mu, \alpha, \theta > 0.$$
	On note que le support de la distribution de Fréchet est pour $x$ supérieur au paramètre de location. 
\end{definition}

\begin{definition}{La distribution Weibull}{}
	La distribution de Weibull standard a la fonction de répartition 
	$$F(x) = G_{2, \alpha}(x) = \exp\left[-\left(-x\right)^{-\alpha}\right], \quad x\leq 0, \alpha < 0.$$
	\tcblower
	Avec les paramètres de location et d'échelle, on a 
	$$F(x) = G_{2, \alpha, \mu, \theta}(x) = \exp\left[-\left(-\frac{x - \mu}{\theta}\right)^{-\alpha}\right], \quad x\leq \mu, \alpha < 0.$$
	
	On note :
	\begin{itemize}
		\item Cette distribution n'est pas la même que la distribution Weibull couramment utilisée en actuariat
		\item Cette distribution a un support pour $x$ inférieur au paramètre de location $\mu$
		\begin{itemize}
			\item Pour cette raison, cette distribution n'est pas utilisée en actuariat.
		\end{itemize}
	\end{itemize}
\end{definition}

\begin{definition}{La distribution de valeurs extrêmes généralisée}{}
	La distribution de valeurs extrêmes généralisée incorpore les trois distributions à valeur extrême comme cas particuliers. L'expression de la fonction de répartition est 
	$$F(x) = \exp \left[-\left(1 + \frac{x}{\alpha}\right)^{-\alpha}\right]$$
	ou (notation équivalente)
	$$F(x) = \exp \left[-\left(1 + \gamma x\right)^{-\frac{1}{\gamma}}\right]$$
	\tcblower
	\begin{itemize}
		\item Pour $\gamma \to 0$, on obtient la distribution Gumbel
		\item Pour $\gamma  > 0$, on obtient la distribution Fréchet
		\item Pour $\gamma < 0$, on obtient la distribution Weibull
	\end{itemize}
\end{definition}

\subsection{Distribution du maximum}

Soit $M_{n}$, la variable aléatoire qui correspond à la valeur maximale de $n$ observations d'une variable aléatoire. Si on a $n$ observations d'une variable aléatoire iid, la fonction de répartition du maximum est 
$$F_{n}(x) = \Pr(M_n \leq x) = \Pr(X_1\leq x, X_2 \leq x, \dots, X_n \leq x) \stackrel{\text{ind}}{=} \prod_{i = 1}^{n} \Pr(X_i\leq x) = \left[F_{X}(x)\right]^n.$$
Les deux premiers moments sont

$$E\left[M_n\right] = \int_{0}^{\infty} [1 - F_{X}^n(x)] dx;$$
$$E\left[M_n^2\right] = 2\int_{0}^{\infty} x[1 - F_{X}^n(x)] dx.$$

Si le nombre $n$ est inconnu, $N$ est une variable aléatoire. On a 
\begin{align*}
F_{M_{N}}(x) &= \Pr(M_{N} \leq x) \\
&= \sum_{n = 0}^{\infty} \Pr(M_{N} \leq x \vert N = n) \Pr(N = n)\\
&= \sum_{n = 0}^{\infty} F_X(x)^n \Pr(N = n)\\
&= P_N\left[F_{X}(x)\right].
\end{align*}

\subsection{Stabilité du maximum des DVE}

On peut montrer que la distribution du maximum des DVE, après normalisation du paramètre de location ou d'échelle, est la même DVE.

\begin{itemize}
	\item \textbf{Gumbel} : $\mu^* = \mu + \theta \ln n$
	\item \textbf{Fréchet} : $\theta^* = \theta n^{1/\alpha}$
\end{itemize}

\subsection{Théorème Fisher-Tippett}

On souhaite évaluer le maximum de $n$ observations lorsque le nombre d'observations approche l'infini. Vu que le maximum est dégénérée. Alors, on étudie une variable normalisée tel que la distribution du maximum n'est pas dégénéré. On requiert une transformation linéaire tel que 
$$\lim\limits_{n\to\infty} F_{n}\left(\frac{x - b_n}{a_n}\right) = G(x) ~ \forall~ x.$$
Si une telle transformation linéaire existe, on peut appliquer le théorème suivant : 

\begin{theoreme}{Théorème Fisher-Tippett}{}
	Si $\left[F\left(\frac{x - b_n}{a_n}\right)\right]^n$ a une distribution non-dégénérée lorsque $n\to\infty$ pour des constantes $a_n$ et $b_n$ qui dépendent de $n$, alors 
	$$\left[F\left(\frac{x - b_n}{a_n}\right)\right]^n \to G(x)$$
	pour $n\to \infty, ~\forall ~ x$, où $G$ est une distribution de valeur extrême. 
\end{theoreme}

Ce théorème nous permet d'étudier le maximum d'une distribution $F$ par $G$, sans nécessairement connaître $F$. 

\begin{itemize}
	\item \textbf{Exponentiel} : $a_n = 1, b_n = \ln n$
	\item \textbf{Pareto} : $a_n = \theta n^{1/\alpha}/\alpha, b = \theta n^{1/\alpha} - \theta$
\end{itemize}

\subsection{Domaine d'attraction du maximum}

\begin{definition}{Domaine d'attraction du maximum}{}
	Le domaine d'attraction du maximum (MDA) pour une distribution $G$ est l'ensemble de toutes les distributions qui ont $G$ comme distribution limite lorsque $n\to \infty$ du maximum normalisé $(M_n - b_n)/a_n$ pour des constantes $a_n$ et $b_n$. 
\end{definition}

Vu qu'on s'intéresse au maximum d'une distribution, le domaine d'attraction du maximum ne dépend que de la queue de la distribution. 

\begin{theoreme}{Caractérisation du MDA par la queue}{}
	Une distribution $F$ appartient au MDA d'une distribution $G_i$ avec constantes de normalisation $a_n$ et $b_n$ si et seulement si 
	$$\lim\limits_{n\to \infty} nS(a_n x + b_n) = -\ln G_i(x).$$
\end{theoreme}

Deux distributions $X$ et $Y$ sont dites équivalentes dans la queue si 
$$\lim\limits_{x\to \infty} \frac{S_X(x)}{S_Y(x)} = c,$$
où $c$ est une constante. Deux distributions équivalentes dans la queue appartiennent au même domaine d'attraction du maximum car $c$ peut absorber les constantes de normalisation. 

\begin{itemize}
	\item Toutes les formes de queue asymptotiques $cx^{-\alpha}$ sont dans le MDA Fréchet. 
	\item Toutes les formes de queue asymptotiques $ke^{-x\theta}$ sont dans le MDA Gumbel. 
\end{itemize}

Toutes les distributions avec l'infini comme borne supérieure appartiennent au MDA Fréchet ou Gumbel.

\begin{theoreme}{}{}
	Une distribution qui a une queue de la forme $S(x) \tilde x^{-\alpha} C(x)$ où $C(x)$ est une fonction qui varie lentement, est dans le MDA Fréchet. 
\end{theoreme}

\subsection{Distributions Pareto généralisées}

Les distributions Pareto généralisées sont reliées avec les distributions à valeur extrême par la relation 
$$W(x) = 1 + \ln G(x)$$
avec la condition supplémentaire que $W(x)$ soit positif ($G(x) > e^{-1}$).

\begin{itemize}
	\item \textbf{Distribution exponentielle} : Gumbel 
	\item \textbf{Distribution Pareto} : Fréchet
	\item \textbf{Distribution beta} : Weibull
\end{itemize}

La distribution Pareto généralisée incorpore toutes les distributions 
$$F(x) = 1 - \left(1 + \frac{x}{\alpha \theta}\right)^{-\alpha}$$
ou parfois écrit sous la forme 
$$1 - \left(1 + \gamma \frac{x}{\theta}\right)^{-1/\gamma}.$$

\subsection{La stabilité de l'excès}

Les distributions Pareto généralisées ont la propriété de stabilité de l'excès. Soit $Y - d \vert X > d$, la variable aléatoire d'excès conditionnelle. Pour la distribution Pareto généralisée, la distribution d'excès conditionnelle  a la même forme que la distribution sous-jacente. 

\subsection{Distribution limite de l'excès}

La variable aléatoire d'excès conditionnelle est définie par $Y = X-d \vert X > d$ avec fonction de distribution 
\begin{align*}
F^* (y) &= F_Y(y) = \Pr(Y \leq y)\\
&= \Pr(X \leq d + y \vert X > d)\\
&= \frac{F(d + y) - F(d)}{1 - F(d)}
\end{align*}

\begin{theoreme}{Théorème Balkema-de Haan-Pickands}{}
	Si, pour des constantes $a_n$ et $b_n$, les distributions conditionnelles d'excès $F^*(a_n x + b_n)$ ont des distributions limites continues lorsque $d$ approche le borne supérieure du support de $X$, alors $$F^*(x) \to W(x)$$ lorsque $d \to \infty, ~\forall~ x$, où $W$ est une distribution Pareto généralisée. 
\end{theoreme}

Lorsque $d$ augmente, la queue droite de la distribution de l'excès converge vers une distribution Pareto généralisée. En pratique, on peut utiliser une distribution Pareto généralisée pour approximer la queue lorsque $d$ est élevé (sans avoir besoin de beaucoup de données). 

\subsection{TVaR des DVA}

Si la distribution d'excès d'une v.a. suit une distribution Pareto généralisée, alors pour $x>d$ on peut écrire la fonction de survie comme
\begin{align}
S_X(x) &= S_X(d) \Pr(X-d > x- d \vert X > d)\nonumber\\
&= S_X(d) S_{Y}^*(y)\label{eq:tail_evd}
\end{align}
où $Y$ est la distribution d'excès conditionnelle $X-d\vert X>d$, $y = x-d$ et $S_Y^*(y)$ est la queue de la distribution d'excès
$$S_Y^*(y) = \left(1 + \gamma \frac{y}{\theta + \gamma d}\right)^{-1/\gamma}.$$
La moyenne de $Y$ est 
$$\frac{\theta}{1-\gamma} + \frac{\gamma}{1 - \gamma}d$$
qui est linéaire en $d$ et existe si $\gamma < 1$. Si $d = VaR_\kappa(X)$, on a 
$$TVaR_p(X) = \frac{VaR_p(X)}{1-\gamma} + \frac{\theta}{1 - \gamma}.$$

Si $d < VaR_\kappa(X)$, on obtient la $VaR$ en inversant \eqref{eq:tail_evd} pour avoir 
$$VaR_\kappa(X) = d + \frac{\theta + \gamma d}{\gamma} \left[ \left( \frac{1 - p}{S_X(d)}\right)^{-\gamma} - 1\right], ~ x > d$$
et
$$TVaR_p(X) = \frac{VaR_p(X)}{1-\gamma} + \frac{\theta}{1 - \gamma}.$$

